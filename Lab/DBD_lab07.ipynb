{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8KCA+Wu+Ve53B9+hwOzTB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lYUo4HLxbrz","executionInfo":{"status":"ok","timestamp":1685955752144,"user_tz":-120,"elapsed":49320,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"a85fe49c-28b3-4b42-eeb5-57840d44934b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=1dd84ad257c5c8cf7bcbc0df3c42a59f0020321531ba735da7af626e95330169\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext \n","from google.colab import files\n","from datetime import datetime\n","import sys\n","import os\n","\n","def edit_value(v):\n","  #retrieve hh\n","  t = v.split('\\t')[1].split(' ')[1].split(':')[0]\n","  return (t,v.split('\\t')[3])\n","\n","def checkFull(line):\n","    # station\\ttimestamp\\tused\\tfree\n","    # 1\\t2008-05-15 12:01:00\\t0\\t18\n","    fields = line.split(\"\\t\")\n","    stationId = fields[0]\n","    freeSlots = int(fields[3])\n","    timestamp = fields[1]\n","    \n","    datetimeObject = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")    \n","    dayOfTheWeek = datetimeObject.strftime(\"%A\")\n","    hour = datetimeObject.hour\n","\n","    if freeSlots == 0:\n","        # The station is full\n","        countTotReadingsTotFull = (1, 1)\n","    else:\n","        countTotReadingsTotFull = (1, 0)\n","        \n","    return ((stationId, dayOfTheWeek, hour), countTotReadingsTotFull)\n","\n","def compare(a, b):\n","  #compare criticality\n","  if a[2]>b[2]:\n","    return a\n","  elif b[2]>a[2]:\n","    return b\n","  \n","  #compare hours\n","  if a[1]<b[1]:\n","    return a\n","  elif b[1]<a[1]:\n","    return b\n","  \n","  #compare days by lexicographical order\n","  if a[0]<b[0]:\n","    return a\n","  return b\n","\n","def extractStationLongLat(line):\n","    fields = line.split(\"\\t\")\n","    \n","    return (fields[0], (fields[1] ,fields[2]) )\n","\n","def formatKMLMarker(pair):\n","    # input\n","    # (stationId, ( (weekday, hour, criticality), (long, lat) ) )\n","    stationId = pair[0]\n","    \n","    weekday = pair[1][0][0]\n","    hour = pair[1][0][1]\n","    criticality = pair[1][0][2]\n","    coordinates = pair[1][1][0]+\",\"+pair[1][1][1]\n","    \n","    result = \"<Placemark><name>\" + stationId + \"</name>\" + \"<ExtendedData>\"\\\n","    + \"<Data name=\\\"DayWeek\\\"><value>\" + weekday + \"</value></Data>\"\\\n","    + \"<Data name=\\\"Hour\\\"><value>\" + str(hour) + \"</value></Data>\"\\\n","    + \"<Data name=\\\"Criticality\\\"><value>\" + str(criticality) + \"</value></Data>\"\\\n","    + \"</ExtendedData>\" + \"<Point>\" + \"<coordinates>\" + coordinates + \"</coordinates>\"\\\n","    + \"</Point>\" + \"</Placemark>\"\n","    \n","    return result\n","\n","if __name__==\"__main__\":\n","  threshold = 0.6\n","\n","  conf = SparkConf().setAppName(\"Spark Lab07\")\n","  sc = SparkContext.getOrCreate(conf=conf)\n","\n","  registerRDD = sc.textFile('registerSample.csv').filter(lambda line: line.startswith('station'))\n","\n","  # filter\n","  registerRDD = registerRDD.filter(lambda l: int(l.split('\\t')[2])!=0 and int(l.split('\\t')[3])!=0)\n","\n","  stationWeekDayHour = registerRDD.map(checkFull)\n","  stationWeekDayHourCounts = stationWeekDayHour.reduceByKey(lambda p1, p2: (p1[0]+p2[0], p1[1]+p2[1]))\n","  stationWeekDayHourCriticality = stationWeekDayHourCounts.mapValues(lambda value: value[1]/value[0])\n","  selectedPairs = stationWeekDayHourCriticality.filter(lambda pair: pair[1]>= threshold)\n","\n","  stationTimeslotCrit = selectedPairs.map(lambda s: (s[0][0],(s[0][1],s[0][2],s[1])))\n","  resultRDD = stationTimeslotCrit.reduceByKey(compare)\n","\n","  stationLocation = sc.textFile('stations.csv').map(extractStationLongLat)\n","  resultLocations = resultRDD.join(stationLocation)\n","  resultKML = resultLocations.map(formatKMLMarker)\n","  sc.stop()"],"metadata":{"id":"rw-rkSviynO2","executionInfo":{"status":"ok","timestamp":1685960191195,"user_tz":-120,"elapsed":1006,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}}},"execution_count":12,"outputs":[]}]}