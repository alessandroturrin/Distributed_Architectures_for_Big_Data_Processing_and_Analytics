{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFLl5jhTNR+SK4dHmzQp5F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"VDRh1_tHAcQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685892434365,"user_tz":-120,"elapsed":45711,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"49096a40-42ca-40f7-808d-2e908cab5887"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=c4f845604bc2ba78eba36cace484533648c5109f6f6a9ee6cff50b2290f5ae96\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext \n","from google.colab import files\n","import sys\n","import os\n","\n","if __name__==\"__main__\":\n","  conf = SparkConf().setAppName(\"Spark Lab05.1\")\n","  sc = SparkContext.getOrCreate(conf=conf)\n","\n","  lines = sc.textFile('input05.txt')\n","\n","  filteredLines = lines.filter(lambda s: s.split('\\t')[0].startswith('ho')) \n","  mappedLines = filteredLines.map(lambda s: float(s.split('\\t')[1]))\n","  countLines = mappedLines.count()\n","  maxFreq = mappedLines.reduce(lambda s1, s2: max(s1, s2))\n","\n","  print('Total lines: ' + str(countLines))\n","  print('Max freq: ' + str(maxFreq))\n","\n","  sc.stop()"],"metadata":{"id":"6lcQtZQeAmw9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685893995930,"user_tz":-120,"elapsed":4622,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"ae4bb16b-f99b-4ff6-cc38-3bce75ce9091"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Total lines: 1913\n","Max freq: 39399.0\n"]}]},{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext \n","from google.colab import files\n","import sys\n","import os\n","\n","#max freq computed in previous task\n","maxFreq = 39399.0\n","\n","if __name__==\"__main__\":\n","  conf = SparkConf().setAppName(\"Spark Lab05.1\")\n","  sc = SparkContext.getOrCreate(conf=conf)\n","\n","  lines = sc.textFile('input05.txt')\n","\n","  filteredLines = lines.filter(lambda s: s.split('\\t')[0].startswith('ho')) \n","  filteredByFreqLines = filteredLines.filter(lambda s: float(s.split('\\t')[1])>0.8*maxFreq)\n","  wordsRDD = filteredByFreqLines.map(lambda s: s.split('\\t')[0])\n","  countLines = filteredByFreqLines.count()\n","\n","  print('Total lines filtered by max freq: ' + str(countLines))\n","  wordsRDD.saveAsTextFile('out')\n","\n","  sc.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4QPmg0LEwV0","executionInfo":{"status":"ok","timestamp":1685894426534,"user_tz":-120,"elapsed":3349,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"d20eb3d9-c913-483b-cfa2-6ecc07195d3c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Total lines filtered by max freq: 3\n"]}]}]}