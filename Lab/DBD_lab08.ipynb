{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmH4pSI4JjZakDGulfs7vA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_GZtjzHKSM5","executionInfo":{"status":"ok","timestamp":1685961985233,"user_tz":-120,"elapsed":51136,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"1ce4d5de-485c-4fbb-e6a2-4e180ebb5016"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=5cd63f4efea01cd38b525f5e47990d7f648da1d570c1c8e91bc483110457a9d1\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["import sys\n","from pyspark import *\n","from pyspark.sql import SparkSession\n","from datetime import datetime\n","\n","# Set input and output folders\n","inputPath  = \"registerSample.csv\" # args[0]\n","inputPath2 = \"stations.csv\" # args[1]\n","threshold  = 0.6 # args[2]\n","outputPath = \"out_Lab8DF\" # args[3]\n","\n","\n","# For the standalone version\n","spark = SparkSession.builder.appName(\"Spark Lab #7 - Template\").getOrCreate()\n","\n","\n","# Read the content of the input file register.csv and store it into a DataFrame\n","# The input file has an header\n","# Schema of the input data:\n","# |-- station: integer (nullable = true)\n","# |-- timestamp: timestamp (nullable = true)\n","# |-- used_slots: integer (nullable = true)\n","# |-- free_slots: integer (nullable = true)\n","inputDF = spark.read.format(\"csv\").option(\"delimiter\", \"\\\\t\").option(\"header\", True).option(\"inferSchema\", True).load(inputPath)\n","\n","\n","# Remove the lines with num free slots=0 && num used slots=0\n","filteredDF = inputDF.filter(\"free_slots<>0 OR used_slots<>0\")\n","\n","\n","# Define a User Defined Function called full(Integer free_slots)\n","# that returns \n","# -- 1 if free_slots is equal to 0\n","# -- 0 if free_slots is greater than 0\n","def fullFunction(free_slots):\n","    if free_slots==0:\n","        return 1\n","    else:\n","        return 0\n","\n","\n","# Define the UDF\n","# name: full\n","# output: integer value\n","spark.udf.register(\"full\", fullFunction)\n","\n","\n","# Define a DataFrame with the following schema:\n","# |-- station: integer (nullable = true)\n","# |-- dayofweek: string (nullable = true)\n","# |-- hour: integer (nullable = true)\n","# |-- fullstatus: integer (nullable = true) - 1 = full, 0 = non-full\n","stationWeekdayHourDF = filteredDF.selectExpr(\"station\", \"date_format(timestamp,'EE') as dayofweek\", \"hour(timestamp) as hour\", \"full(free_slots) as fullstatus\")\n","\n","\n","# Define one group for each combination \"station, dayofweek, hour\"\n","rgdStationWeekdayHourDF = stationWeekdayHourDF.groupBy(\"station\", \"dayofweek\", \"hour\")\n","\n","\n","# Compute the criticality for each group (station, dayofweek, hour),\n","# i.e., for each pair (station, timeslot)\n","# The criticality is equal to the average of fullStatus\n","stationWeekdayHourCriticalityDF = rgdStationWeekdayHourDF.agg({\"fullStatus\": \"avg\"}).withColumnRenamed(\"avg(fullStatus)\", \"criticality\")\n","\n","# Select only the lines with criticality > threshold\n","selectedPairsDF = stationWeekdayHourCriticalityDF.filter(\"criticality>\"+str(threshold))\n","\n","# Read the content of the input file stations.csv and store it into a DataFrame\n","# The input file has an header\n","# Schema of the input data:\n","# |-- id: integer (nullable = true)\n","# |-- longitude: double (nullable = true)\n","# |-- latitude: double (nullable = true)\n","# |-- name: string (nullable = true)\n","stationsDF = spark.read.format(\"csv\").option(\"delimiter\", \"\\\\t\").option(\"header\", True).option(\"inferSchema\", True).load(inputPath2)\n","\n","# Join the selected critical \"situations\" with the stations table to\n","# retrieve the coordinates of the stations\n","selectedPairsCoordinatesDF = selectedPairsDF.join(stationsDF, selectedPairsDF.station == stationsDF.id)\n","\n","# Sort the content of the DataFrame\n","selectedPairsIdCoordinatesCriticalityDF = selectedPairsCoordinatesDF.selectExpr(\"station\", \"dayofweek\", \"hour\", \"longitude\", \"latitude\", \"criticality\").sort(selectedPairsCoordinatesDF.criticality.desc(),      selectedPairsCoordinatesDF.station,      selectedPairsCoordinatesDF.dayofweek,      selectedPairsCoordinatesDF.hour)\n","\n","selectedPairsIdCoordinatesCriticalityDF.write.format(\"csv\").option(\"header\", True).save(outputPath)\n","\n","# Close the Spark session\n","spark.stop()\n","\n"],"metadata":{"id":"i3W7dH9TKWeW","executionInfo":{"status":"ok","timestamp":1685963219069,"user_tz":-120,"elapsed":11644,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import sys\n","from pyspark import *\n","from pyspark.sql import SparkSession\n","from datetime import datetime\n","\n","#################################\n","# Task 1\n","#################################\n","\n","# Set input and output folders\n","inputPath  = \"registerSample.csv\" # args[0]\n","inputPath2 = \"stations.csv\" # args[1]\n","threshold  = 0.6 # args[2]\n","outputPath = \"out_Lab8DF_SQL\" # args[3]\n","\n","# For the standalone version\n","spark = SparkSession.builder.appName(\"Spark Lab #7 - Template\").getOrCreate()\n","\n","\n","# Read the content of the input file register.csv and store it into a DataFrame\n","# The input file has an header\n","# Schema of the input data:\n","# |-- station: integer (nullable = true)\n","# |-- timestamp: timestamp (nullable = true)\n","# |-- used_slots: integer (nullable = true)\n","# |-- free_slots: integer (nullable = true)\n","inputDF = spark.read.format(\"csv\").option(\"delimiter\", \"\\\\t\").option(\"header\", True).option(\"inferSchema\", True).load(inputPath)\n","\n","\n","# Assign the “table name” readings to inputDF\n","inputDF.createOrReplaceTempView(\"readings\")\n","\n","# Define a User Defined Function called full(Integer free_slots)\n","# that returns \n","# -- 1 if free_slots is equal to 0\n","# -- 0 if free_slots is greater than 0\n","def fullFunction(free_slots):\n","    if free_slots==0:\n","        return 1\n","    else:\n","        return 0\n","\n","# Define the UDF\n","# name: full\n","# output: integer value\n","spark.udf.register(\"full\", fullFunction)\n","\n","# Select only the lines with free_slots<>0 or used_slots<>0 and then \n","# compute the criticality for each group (station, dayofweek, hour) (i.e., for each pair (station, timeslot))\n","# and finally select only the groups with criticality>threshold.\n","#\n","# The criticality of each group is equal to the average of full(free_slots)\n","# The schema of the returned dataset is:\n","# |-- station: integer (nullable = true)\n","# |-- dayofweek: string (nullable = true)\n","# |-- hour: integer (nullable = true)\n","# |-- criticality: double (nullable = true)\n","selectedPairsDF = spark.sql(\"\"\"SELECT station, date_format(timestamp,'EE') as dayofweek, \n","hour(timestamp) as hour, avg(full(free_slots)) as criticality \n","FROM readings \n","WHERE free_slots<>0 OR used_slots<>0\n","GROUP BY station, date_format(timestamp,'EE'), hour(timestamp)\n","HAVING avg(full(free_slots))>\"\"\"+str(threshold))\n","\n","\n","# Assign the “table name” criticals to selectedPairsDF\n","selectedPairsDF.createOrReplaceTempView(\"criticals\")\n","\n","# Read the content of the input file stations.csv and store it into a DataFrame\n","# The input file has an header\n","# Schema of the input data:\n","# |-- id: integer (nullable = true)\n","# |-- longitude: double (nullable = true)\n","# |-- latitude: double (nullable = true)\n","# |-- name: string (nullable = true)\n","stationsDF = spark.read.format(\"csv\").option(\"delimiter\", \"\\\\t\").option(\"header\", True).option(\"inferSchema\", True).load(inputPath2)\n","\n","\n","# Assign the “table name” stations to stationsDF\n","stationsDF.createOrReplaceTempView(\"stations\")\n","\n","\n","# Join the selected critical \"situations\" with the stations table to\n","# retrieve the coordinates of the stations.\n","# Select only the column station, longitude, latitude and criticality\n","# and sort records by criticality (desc), station (asc)\n","selectedPairsIdCoordinatesCriticalityDF = spark.sql(\"\"\"SELECT station, dayofweek, hour, \n","longitude, latitude, criticality\n","FROM criticals, stations\n","WHERE criticals.station = stations.id\n","ORDER BY criticality DESC, station, dayofweek, hour\"\"\")\n","\n","selectedPairsIdCoordinatesCriticalityDF.write.format(\"csv\").option(\"header\", True).save(outputPath)\n","\n","\n","# Close the Spark session\n","spark.stop()\n","\n","\n","\n"],"metadata":{"id":"xeB7ZKhuPUmT","executionInfo":{"status":"ok","timestamp":1685963283822,"user_tz":-120,"elapsed":5093,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}}},"execution_count":12,"outputs":[]}]}