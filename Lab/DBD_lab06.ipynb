{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqZ/EmxbOfTGtRwLvvgLCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exfZNEd9JIRZ","executionInfo":{"status":"ok","timestamp":1685894572550,"user_tz":-120,"elapsed":46293,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"a4344a97-7fcb-4baf-d7c1-85c2a0cc1b47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=8b5dee4b51ae244ba904e705cdb831db56a8939e6262e59331b145845cb85259\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext \n","from google.colab import files\n","import sys\n","import os\n","\n","def extractPairsOfProducts(transaction):\n","\n","    products = list(transaction)\n","\n","    returnedPairs = []\n","    \n","    for product1 in products:\n","        for product2 in products:\n","            if product1<product2:\n","                returnedPairs.append( ((product1, product2), 1) )\n","                \n","    return returnedPairs\n","    \n","\n","if __name__==\"__main__\":\n","  conf = SparkConf().setAppName(\"Spark Lab06\")\n","  sc = SparkContext.getOrCreate(conf=conf)\n","\n","  #task 1\n","  lines = sc.textFile(\"ReviewsSample.csv\")\n","  skipped = lines.filter(lambda line: line.startswith(\"Id,\")==False)\n","  setKeyValLines = skipped.map(lambda s: (s.split(',')[2], s.split(',')[1])).distinct()\n","  userIdByKey = setKeyValLines.groupByKey()\n","  finalRDD = userIdByKey.mapValues(lambda l: list(l))\n","\n","  print(\"UserID - List [ProductID]\\n\" + str(finalRDD.collect()) + \"\\n\")\n","\n","\n","  #task 2\n","  pairsOfProductsOneRDD = finalRDD.values().flatMap(extractPairsOfProducts)\n","  setKeyValReducer = pairsOfProductsOneRDD.reduceByKey(lambda c1, c2: c1+c2)\n","\n","  #task 3\n","  filteredKeyVal = setKeyValReducer.filter(lambda inTuple: inTuple[1]>1)\n","  sortedKeyVal = filteredKeyVal.sortBy(lambda inputTuple: inputTuple[1], False).cache()\n","  .saveAsTextFile(outputPath)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UCxo8uSJOGO","executionInfo":{"status":"ok","timestamp":1685896433363,"user_tz":-120,"elapsed":3162,"user":{"displayName":"Alex Turrin","userId":"07324947984286734629"}},"outputId":"80acb635-5154-4d2c-e41d-28bc0c7be8c3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["UserID - List [ProductID]\n","[('A2', ['B1', 'B3', 'B5']), ('A4', ['B1', 'B3', 'B4', 'B5']), ('A3', ['B3']), ('A1', ['B2']), ('A5', ['B5', 'B1', 'B3'])]\n","\n","Frequencies of items pairs:\n","[(('B1', 'B3'), 3), (('B1', 'B5'), 3), (('B3', 'B5'), 3)]\n","\n"]}]}]}